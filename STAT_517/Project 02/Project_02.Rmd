---
title: "Does Size Matter? (Estimation of Banana Weight with a Regression Modeling Approach)"
author: "Scott Graham, Kaisa Roggeveen"
date: "February 13, 2018"
header-includes:
  - \newcommand{\Prob}{\operatorname{P}}
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\se}{\operatorname{se}}
  - \newcommand{\re}{\operatorname{re}}
  - \newcommand{\ybar}{{\overline{Y}}}
  - \newcommand{\phat}{{\hat{p}}}
  - \newcommand{\that}{{\hat{T}}}
  - \newcommand{\med}{{\tilde{Y}}}
  - \newcommand{\Logit}{{\operatorname{Logit}}}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(pander, warn.conflicts = FALSE, quietly = TRUE)
library(memisc, warn.conflicts = FALSE, quietly = TRUE)
library(DAAG, warn.conflicts = FALSE, quietly = TRUE)
library(glmnet, warn.conflicts = FALSE, quietly = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(magrittr, warn.conflicts = FALSE, quietly = TRUE)
library(ggfortify, warn.conflicts = FALSE, quietly = TRUE)
library(knitr, warn.conflicts = FALSE, quietly = TRUE)

set.seed(5609)

theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
        ,fill = NA
      )
    )

cv.adaptive.glmnet <- 
  function(
    x
    ,y
    ,alpha = 1
    ,gamma = 1
    ,weights
    ,nfolds = 10
    ,parallel = FALSE
    ,...
  ){
    cv.alasso.ridge <- 
      cv.glmnet(
        x = x
        ,y = y
        ,alpha = 0
        ,nfolds = nfolds
        ,parallel = parallel
      )
    
    cv.alasso.weights <- 1 / abs(coef(object = cv.alasso.ridge, s = "lambda.min", exact = TRUE)[-1, 1])^(gamma)
    
    cv.alasso.model <-   
      cv.glmnet(
        x = x
        ,y = y
        ,alpha = alpha
        ,penalty.factor = cv.alasso.weights
      )
    cv.alasso.model
  }

banana_data <-
  "mybanana.txt" %>% 
  read_tsv()

banana_data <-
  banana_data %>% 
  mutate_at(
    .vars = vars(Weight:Circumference)
    ,.funs = funs(log = log)
  )
```

# Summary


# Introduction
The purpose of this study was to determine the most effective regression model to predict the weight of a banana using external measurements. This study also demonstrated multiple techniques for developing regression models. These models were then examined to demonstrate their effectiveness at creating regression models.

# Data Collection
First a small sample set bananas were purchased from the Real Canadian Superstore. The weight, length, diameter and circumference were then calculated using a scale and a ruler. 

```{r Sample Size, cache=TRUE}
rsquare <- c()
for(i in 1:1000){
  banana_cor <- 
    banana_data %>%
    sample_n(10) %>% 
    select(
      Weight
      ,Radius
      ,Length
      # ,Circumference
    ) %>% 
    cor()
  
  xy_vec <- banana_cor[2:3, 1]
  C_mat <- banana_cor[2:3, 2:3]
  
  rsquare[i] <- t(xy_vec) %*% solve(C_mat) %*% xy_vec
}
rsquare %>% 
  summary() %>% 
  pander()
```

In order to determine the minimum sample size needed, random sample sizes of 10 were generated using radius and length as the predictors. The correlation of the random sample sizes were calculated and a matrix of the correlations were generated. The value of the squared population multiple correlation coefficients with two predictor variables was then calculated and determined to be approximately `r round(mean(rsquare), 4)`. From this the minimum sample size required was then determined from the table from Gregory T. Knofcznski's Sample Size When Using Multiple Linear Regression for Prediction, the minimum sample size was determined to be between 15 and 35, therefore the minimum number of bananas required was finalized at 24 bananas. 

# Analysis
To begin analysis a model using all predictor variables was created. In this case the density of the banana is assumed to be a constant.

Let:
$$
  W = \text{Weight (g), }
  L = \text{Length (mm), }
  R = \text{Radius (mm)}
$$
Then:
$$
  \log(W) = 
  \beta_{0} + \beta_{1}\log(L) + \beta_{2}\log(R) + \beta_{3}\log(C) \implies
  W = 
  e^{\beta_{0}} \times L^{\beta_{1}} \times R^{\beta_{2}} \times C^{\beta_{3}}
$$
```{r Reg 01}
banana_reg_01 <-
  banana_data %>% 
  lm(
    Weight_log ~ Length_log + Radius_log + Circumference_log
    ,data = .
  )
pander(summary(banana_reg_01))
```

In the second model the predictor variable, circumference, was removed. This is because $C=2\pi R$.
$$
  \log(W) = \beta_{0} + \beta_{1}\log(L) + \beta_{2}\log(R)
$$
```{r Reg 02}
banana_reg_02 <-
  banana_data %>% 
  lm(
    Weight_log ~ Length_log + Radius_log
    ,data = .
  )
pander(summary(banana_reg_02))
```

The third model considered the predictor, length.
$$
  \log(W) = \beta_{0} + \beta_{1}\log(L)
$$
```{r Reg 03}
banana_reg_03 <-
  banana_data %>% 
  lm(
    Weight_log ~ Length_log
    ,data = .
  )
pander(summary(banana_reg_03))
```

The fourth model considered only one predictor, radius.
$$
  \log(W) = \beta_{0} + \beta_{2}\log(R)
$$
```{r Reg 04}
banana_reg_04 <-
  banana_data %>% 
  lm(
    Weight_log ~ Radius_log
    ,data = .
  )
pander(summary(banana_reg_04))
```

```{r Models}
# mtable123 <-
#   mtable(
#     "M1" = banana_reg_01
#     ,"M2" = banana_reg_02
#     ,"M3" = banana_reg_03
#     ,"M4" = banana_reg_04
#     ,summary.stats = c('R-squared','F','p','N')
#   )
# toLatex(mtable123)

pander(anova(banana_reg_02, banana_reg_01), caption = "foo")
pander(anova(banana_reg_03, banana_reg_01))
pander(anova(banana_reg_03, banana_reg_02))


banana_ind <- 
  banana_data %>% 
  select(
    Length
    ,Radius
    ,Circumference
    ,Length_log
    ,Radius_log
    ,Circumference_log
  ) %>% 
  as.matrix()

banana_dep <- 
  banana_data %>% 
  select(
    Weight
    ,Weight_log
  ) %>% 
  as.matrix()

banana_lasso_cvfit_01 <-
  cv.adaptive.glmnet(
    x = banana_ind
    ,y = banana_dep[, "Weight_log"]
    ,alpha = 1
  )

autoplot(banana_lasso_cvfit_01)
banana_lasso_cvfit_01 %>% 
  coef(s = "lambda.min") %>% 
  as.matrix() %>% 
  kable()
banana_lasso_cvfit_01 %>% 
  coef(s = "lambda.1se") %>% 
  as.matrix() %>% 
  kable()

# banana_lasso_cvfit_02 <-
#   cv.adaptive.glmnet(
#     x = banana_ind
#     ,y = banana_dep[, "Weight"]
#     ,alpha = 1
#   )
# 
# autoplot(banana_lasso_cvfit_02)
# banana_lasso_cvfit_02 %>% 
#   coef(s = "lambda.min") %>% 
#   as.matrix() %>% 
#   kable()
# banana_lasso_cvfit_02 %>% 
#   coef(s = "lambda.1se") %>% 
#   as.matrix() %>% 
#   kable()
```
# Recommendations
It can be determiend that the best way to predict the weight of a banana is by measuring the radius of the banana. The model that is then used for banana weight prediction is the following:
$$
  \log(W) = \beta_{0} + \beta_{1}\log(R)
$$
$$
  \log(W) = 0.3046 + 1.669\log(R)
$$
The predictor variable radius, was more significantly correlated to the weight in comparison to circumference and length. 

# Appendix


```{r Visualizations}
banana_data %>%
  select(
    -c(
      ID
      ,Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  # select(-ID) %>% 
  cor() %>% 
  as.data.frame() %>%  
  rownames_to_column() %>% 
  as.tibble() %>% 
  gather(
    key = Column
    ,value = Correlation
    ,-rowname
  ) %>% 
  rename(Row = rowname) %>% 
  ggplot(
    aes(
      x = Column
      ,y = Row
      ,fill = Correlation
    )
  ) +
  geom_raster() +
  scale_fill_distiller(
    type = "div"
    ,palette = "RdBu"
    ,limits = c(-1, 1)
  ) +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)
    ,axis.title.x = element_blank()
    ,axis.title.y = element_blank()
    ,panel.grid = element_blank()
  )

banana_tidy <- 
  banana_data %>% 
  select(
    -c(
      Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  gather(
    key = "Type"
    ,value = "Measurement"
    ,-ID
  )

banana_tidy %>% 
  ggplot(aes(x = Measurement, colour = Type)) +
  geom_histogram(
    aes(y = ..density..)
    ,alpha = 0
    ,binwidth = function(x) nclass.FD(x)
  ) +
  geom_density() +
  facet_wrap(
    ~ Type
    ,scales = "free"
  ) +
  scale_colour_brewer(
    palette = "Dark2"
    ,type = "qual"
  )

banana_data %>% 
  select(
    -c(
      Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  gather(
    key = "Type"
    ,value = "Measurement"
    ,-ID
    ,-Weight
  ) %>% 
  ggplot(
    aes(
      x = Measurement
      ,y = Weight
      ,colour = Type
    )
  ) +
  geom_smooth(
    method = "loess"
    ,se = FALSE
  ) +
  geom_smooth(
    method = "lm"
    ,se = FALSE
  ) +
  geom_point() +
  facet_wrap(
    ~ Type
    ,scales = "free_x"
  ) +
  scale_colour_brewer(
    palette = "Set2"
    ,type = "qual"
  )
```


# Cross Validation
```{r CV}
# banana_data %>% 
#    cv.lm(
#      Weight_log ~ Length_log + Radius_log
#    )

banana_reg_cv <- 
  banana_data %>%
  cv.lm(
    Weight_log ~ Length_log + Radius_log
    ,plotit = FALSE
  )
banana_reg_cv %>% 
  mutate(
    `CV Residual` = (cvpred) - (Weight_log)
    ,Residual = Predicted - Weight_log
  )
#Calculate the Mean Absolute Error

#Caculate the Mean Absolute Percentage Error

#Comparison MAE and MPAE values
```

## MAE
```{r MAE}
PvsA <- 
  tibble(
    Predicted = exp(predict(banana_lasso_cvfit_01, newx = banana_ind, s = "lambda.min")[, "1"])
    ,Actual = banana_dep[, "Weight"]
  ) %>% 
  mutate(
    AE = abs(Actual - Predicted)
    ,PAE = AE/Actual
  ) 
PvsA %>% 
  summarize(
    MAE = mean(AE)
    ,MPAE = mean(PAE)
  )
```
